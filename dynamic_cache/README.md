# Adaptive Semantic Cache System for LLM Cost Optimization

A production-ready, dynamic semantic caching system that reduces LLM costs through intelligent query matching, adaptive thresholds, and value-based cache management.

## ğŸ¯ Overview

This system demonstrates **real cost reduction** (not a toy cache) by:
- **Reducing redundant LLM calls** using FAISS-powered semantic similarity
- **Tracking token usage and cost savings** with detailed metrics
- **Real-time monitoring** via interactive Streamlit dashboard
- **Dynamically deciding what to cache** based on value scoring
- **Maintaining cache freshness** via intelligent eviction
- **Adapting thresholds and policies** based on observed usage patterns

## ğŸ—ï¸ Architecture

```
User Query
   â†“
Normalize + Embed Query (models/embedding-001)
   â†“
FAISS Semantic Search (cosine similarity, 768-dim)
   â†“
Adaptive Similarity Threshold
   â†“
CACHE HIT â”€â”€â†’ Return Cached Response + Update Metrics
CACHE MISS â”€â†’ Call LLM (gemini-2.0-flash)
                â†“
            Cost Tracking
                â†“
        Adaptive Cache Decision
                â†“
        Store (if high-value)
                â†“
        Eviction (if cache full)
```